# 死链对网站的影响、产生过程、解决办法汇总

## 大量死链对搜索引擎来讲，有什么影响?
1、几乎很多站都避免不了存在死链接，但死链率过高，会影响搜索引擎对网站的评分。

2、搜索引擎对每个网站每天抓取的频次是限额的，若网站存在大量死链，会浪费掉抓取配额并影响正常页面的抓取。

3、过多死链对网站用户来讲也是体验不好的表现。

## 百度站长工具中有一个抓取异常，那么这个工具里面的异常数据是如何产生的呢?
1、网站内部添加了错误的内部链接
编辑失误或者程序员大意让页面产生了不存在页面的 URL。

2、原来正常页面因为程序或页面调整或改版无法打开
因为程序的一个改动，导致了某些正常页面无法打开。

3、网站短暂无法访问
因为服务器、空间或程序问题导致网站无法访问，抓取异常中会出现大量的 500 错误页面。

4、外部链接错误
用户或者站长在站外发布了错误 URL，蜘蛛爬取后产生错误页面;别的网站复制或采集了你们含有错误链接的页面;有些垃圾网站自动生成的静态搜索结果页，如 www。8875。org/desc/3715714444.html 这个页面中出现的很多链接在 html 前面有“...”。

5、爬虫提取了不完整的 URL
个别爬虫在提取页面 URL 的时候，只提取部分 URL 或者把正常的 URL 后面的文字或字符也提取进去了。

6、网站改版或管理员删除页面
网站改版过程中处理不当导致部分老页面无法访问，或者网站管理员删除被黑、广告、过时、被灌水页面。

## 出现了上述情况，我们该如何去解决
1、修复错误页面 抓取异常中的很多错误页面是因为程序员大意或者我们程序问题导致的，本应该是正常页面，因为失误导致无法访问，对于此类页面，第一时间进行修复。

2、提交死链接 但必定很多错误页面是本不应该存在的，那么我们要想办法获取此类页面 URL，获取方式主要有三种：
（1） 百度站长工具--抓取异常--找不到的页面--复制数据 [修正：此处死链不需我们提交了，百度站长工具自动提交死链];
（2） 管理员在删除页面的时候手动或者程序自动保存被删页面 URL;
（3） 使用相关爬虫软件，爬取整站获取死链，如 Xenu 。

然后将上述数据合并并删除重复项(excel 表格中能实现删重，wps 表格更容易操作)，然后再复制所有 URL 通过 HTTP 状态批量查询工具查询下[这个不错 pl.soshoulu.com/webspeed.aspx]，剔除掉非 404 返回码页面。

然后整理上面处理好的数据粘贴到网站根目录中的一个文档中，再把文档地址提交到 百度站长工具--网页抓取--死链提交--添加新数据--填写死链文件地址。

3、在 robots 中屏蔽抓取
若大量的错误 URL 存在一定的规律，可以在 robots 文件中写一个规则禁止蜘蛛程序抓取此类链接，但前提一定要照顾好正常页面，避免屏蔽规则误伤正常页面，比如你的网站全部是静态 URL，那么如果错误链接中含有?的话，规则写成 Disallow:/*?*，如果错误链接中有/id...html 的话，规则写成 Disallow:/*...* 。

robots 文件添加完规则后，一定要去百度站长的 robots 工具 进行校验，将指定的错误页面放进去，看看是否封禁成功，再把正常的页面放进去看看是否被误封。

## 相关注意事项：
1、在百度站长工具中提交死链前，一定要确保提交的死链数据不存在活链接，或者存在 HTTP 状态码非 404 页面。一旦存在活链，会显示提交失败导致无法删除。

2、由于很多网站程序问题，很多打不开的页面返回码并非 404，这是一个大问题，比如明明是打不开的页面返回码是 301、200、500，如果是 200 的话，会导致网站中出现不同 URL 获取相同内容。比如我的一个站，社区的帖子删除后，返回码却是 500，后期发现了，立马进行了处理。大家尽量找出所有错误 URL 格式，.并设置打开后 HTTP 状态码为 404。

3、把所有的错误页面找到后，一定要寻找此类页面 URL 相同特征，并且和正常页面的特征有区分，写对应规则到 robots 文件中，禁止蜘蛛抓取，就算你已经在站长工具中提交了死链，也建议 robots 封禁抓取。

4、robots 只能解决蜘蛛不再抓取此类页面，但解决不了已经抓取页面的快照的删除问题，若你的网站被黑，删除了被黑页面，除了 robots 封禁被黑页面外，还应该将这些页面提交死链。提交死链是删除被黑页面快照的最快办法。[站长社区-版主 ccutu]